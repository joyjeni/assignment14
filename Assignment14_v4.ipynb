{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment14_v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwa59cXgdshz",
        "colab_type": "code",
        "outputId": "b688cbbb-d739-4d32-9543-22b90aeaeba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i00Og0AITpUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "from glob import glob\n",
        "from pprint import pprint\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "import sys\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwHUm1Xudu09",
        "colab_type": "code",
        "outputId": "bfb078ba-105f-4223-b5cb-c38ac1ed1518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.test.gpu_device_name() "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQR_JVd2du4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/eva/session14/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxbibxQUJ0pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS=tf.app.flags.FLAGS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvpyArUdMl1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic model parameters.\n",
        "tf.app.flags.DEFINE_integer('batch_size', 128,\n",
        "                            \"\"\"Number of images to process in a batch.\"\"\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-GLGbhaMl4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOWER_NAME = 'tower'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd6xsZpVMly7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu-FYEydtpre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JvWgtdUYrv0",
        "colab_type": "text"
      },
      "source": [
        "### Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_5gtx98YtX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
        "\n",
        "\n",
        "def maybe_download_and_extract(data_dir='data'):\n",
        "    \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
        "    dest_directory = data_dir\n",
        "    os.makedirs(dest_directory, exist_ok=True)\n",
        "    filename = DATA_URL.split('/')[-1]\n",
        "    filepath = os.path.join(dest_directory, filename)\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        def _progress(count, block_size, total_size):\n",
        "            sys.stdout.write('\\r>> Downloading {} {:.2%}%'.format(\n",
        "                filename, float(count * block_size) / float(total_size) * 100.0))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
        "        statinfo = os.stat(filepath)\n",
        "        print('\\nSuccessfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
        "\n",
        "    extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
        "    if not os.path.exists(extracted_dir_path):\n",
        "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S8ihRHHYt6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size =128\n",
        "data_dir='data/cifar/'\n",
        "model_dir=\"models/cifar\"\n",
        "NUM_CLASSES=10\n",
        "\n",
        "mode = tf.estimator.ModeKeys.TRAIN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD01SJZMYu8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _variable_on_cpu(name, shape, initializer):\n",
        "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
        "  Args:\n",
        "    name: name of the variable\n",
        "    shape: list of ints\n",
        "    initializer: initializer for Variable\n",
        "  Returns:\n",
        "    Variable Tensor\n",
        "  \"\"\"\n",
        "  with tf.device('/gpu:0'):\n",
        "    dtype = tf.float32\n",
        "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
        "  return var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvJGgNfZfuIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
        "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
        "  Note that the Variable is initialized with a truncated normal distribution.\n",
        "  A weight decay is added only if one is specified.\n",
        "  Args:\n",
        "    name: name of the variable\n",
        "    shape: list of ints\n",
        "    stddev: standard deviation of a truncated Gaussian\n",
        "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
        "        decay is not added for this Variable.\n",
        "  Returns:\n",
        "    Variable Tensor\n",
        "  \"\"\"\n",
        "  dtype = tf.float32\n",
        "  var = _variable_on_cpu(\n",
        "      name,\n",
        "      shape,\n",
        "      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
        "  if wd is not None:\n",
        "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "    tf.add_to_collection('losses', weight_decay)\n",
        "  return var\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcAd89cXHzBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _activation_summary(x):\n",
        "  \"\"\"Helper to create summaries for activations.\n",
        "  Creates a summary that provides a histogram of activations.\n",
        "  Creates a summary that measures the sparsity of activations.\n",
        "  Args:\n",
        "    x: Tensor\n",
        "  Returns:\n",
        "    nothing\n",
        "  \"\"\"\n",
        "  # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
        "  # session. This helps the clarity of presentation on tensorboard.\n",
        "  tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
        "  tf.summary.histogram(tensor_name + '/activations', x)\n",
        "  tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmd9SDdiYx5X",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ZW7WzzYwvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def l2_regularizer(wd):\n",
        "    def _l2_regularizer(var, name=None):\n",
        "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=name)\n",
        "        tf.add_to_collection('losses', weight_decay)\n",
        "        return weight_decay\n",
        "    return _l2_regularizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPHmVx07Y5ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_block(inputs, filters, kernel_size, name):\n",
        "    \"\"\"Builds a ConvBlock as seen in the section 2.3 slides.\n",
        "\n",
        "    Args:\n",
        "        inputs:\n",
        "        filters: {N}\n",
        "        kernel_size: {KS}\n",
        "        name:\n",
        "\n",
        "    Assumes we are using the default data format: NHWC\n",
        "\n",
        "    For the stuff below, let:\n",
        "        PS = pool_size\n",
        "        KS = kernel_size\n",
        "        S = stride for pooling\n",
        "\n",
        "    Output shapes can be computed with standard formulas, given stride=1:\n",
        "        Output height H <= (H - KS + 1 - PS)/S + 1\n",
        "        Output width: W <= (W - KS + 1 - PS)/S + 1\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name, 'conv_block'):\n",
        "        x = tf.layers.Conv2D(\n",
        "            filters, kernel_size,\n",
        "            padding='same',\n",
        "            use_bias=False)(inputs)\n",
        "\n",
        "            \n",
        "        x = tf.layers.BatchNormalization(\n",
        "            epsilon=1e-5,\n",
        "            fused=True,\n",
        "            name='batch_norm')(x, training=True)\n",
        "        x = tf.nn.relu(x, name='relu')\n",
        "        x = tf.layers.MaxPooling2D(\n",
        "            pool_size=3,\n",
        "            strides=2,\n",
        "            padding='same',\n",
        "            name='max_pool')(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6KKBAGSY91x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "def inference(image_batch, batch_size=128):  # input batch of images and size\n",
        "    \"\"\"Build the CIFAR-10 model.\n",
        "    \n",
        "    Args:\n",
        "      image_batch: Images returned from distorted_inputs() or inputs().\n",
        "      batch_size: (int) number of examples per batch.\n",
        "    \n",
        "    Returns:\n",
        "      Logits.\n",
        "    \"\"\"\n",
        "\n",
        "    # ## Prep\n",
        "    # conv1\n",
        "\n",
        "    with tf.variable_scope('conv1') as scope:\n",
        "        kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3,\n",
        "                64], stddev=5e-2, wd=None)\n",
        "        conv1 = tf.nn.conv2d(image_batch, kernel, [1, 1, 1, 1],\n",
        "                             padding='SAME')  # conv1\n",
        "        biases1 = _variable_on_cpu('biases1', [64],\n",
        "                                   tf.constant_initializer(0.0))\n",
        "        pre_activation1 = tf.nn.bias_add(conv1, biases1)\n",
        "        conv1 = tf.nn.relu(pre_activation1, name=scope.name)  # Relu 1\n",
        "        _activation_summary(conv1)\n",
        "\n",
        "        # norm1\n",
        "\n",
        "        norm1 = tf.nn.lrn(\n",
        "            conv1,\n",
        "            4,\n",
        "            bias=1.0,\n",
        "            alpha=0.001 / 9.0,\n",
        "            beta=0.75,\n",
        "            name='norm1',\n",
        "            )\n",
        "\n",
        "#####Layer 1\n",
        "\n",
        "# conv2\n",
        "\n",
        "    with tf.variable_scope('conv2') as scope:\n",
        "        kernel = _variable_with_weight_decay('weights', shape=[5, 5,\n",
        "                64, 64], stddev=5e-2, wd=None)\n",
        "        conv2 = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME'\n",
        "                             )  # Conv2\n",
        "        biases2 = _variable_on_cpu('biases2', [64],\n",
        "                                   tf.constant_initializer(0.1))\n",
        "        pre_activation2 = tf.nn.bias_add(conv2, biases2)\n",
        "        conv2 = tf.nn.relu(pre_activation2, name=scope.name)  # Relu 2\n",
        "        _activation_summary(conv2)\n",
        "\n",
        "        # norm2\n",
        "\n",
        "        norm2 = tf.nn.lrn(\n",
        "            conv2,\n",
        "            4,\n",
        "            bias=1.0,\n",
        "            alpha=0.001 / 9.0,\n",
        "            beta=0.75,\n",
        "            name='norm2',\n",
        "            )\n",
        "\n",
        "        # pool1\n",
        "\n",
        "        pool1 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1,\n",
        "                               2, 2, 1], padding='SAME', name='pool1')\n",
        "\n",
        "    # # res1 Layer 1\n",
        "\n",
        "    with tf.variable_scope('conv3') as scope:\n",
        "        kernel = _variable_with_weight_decay('weights', shape=[5, 5,\n",
        "                64, 64], stddev=5e-2, wd=None)\n",
        "        conv3 = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME'\n",
        "                             )  # Conv 3\n",
        "        biases3 = _variable_on_cpu('biases3', [64],\n",
        "                                   tf.constant_initializer(0.1))\n",
        "        pre_activation3 = tf.nn.bias_add(conv3, biases3)\n",
        "        conv3 = tf.nn.relu(pre_activation3, name=scope.name)  # Relu 3\n",
        "        _activation_summary(conv3)\n",
        "\n",
        "        # norm3\n",
        "\n",
        "        norm3 = tf.nn.lrn(\n",
        "            conv3,\n",
        "            4,\n",
        "            bias=1.0,\n",
        "            alpha=0.001 / 9.0,\n",
        "            beta=0.75,\n",
        "            name='norm3',\n",
        "            )\n",
        "\n",
        "    # ## Resnet 2 Layer 1\n",
        "\n",
        "    with tf.variable_scope('conv4') as scope:\n",
        "        kernel = _variable_with_weight_decay('weights', shape=[5, 5,\n",
        "                64, 64], stddev=5e-2, wd=None)\n",
        "        conv4 = tf.nn.conv2d(norm3, kernel, [1, 1, 1, 1], padding='SAME'\n",
        "                             )  # Conv2\n",
        "        biases4 = _variable_on_cpu('biases4', [64],\n",
        "                                   tf.constant_initializer(0.1))\n",
        "        pre_activation4 = tf.nn.bias_add(conv4, biases4)\n",
        "        conv4 = tf.nn.relu(pre_activation4, name=scope.name)  # Relu 2\n",
        "        _activation_summary(conv4)\n",
        "\n",
        "        # norm2\n",
        "\n",
        "        norm4 = tf.nn.lrn(\n",
        "            conv4,\n",
        "            4,\n",
        "            bias=1.0,\n",
        "            alpha=0.001 / 9.0,\n",
        "            beta=0.75,\n",
        "            name='norm2',\n",
        "            )\n",
        "\n",
        "        # # Do addition\n",
        "\n",
        "        # pool4\n",
        "\n",
        "        #pool4 = tf.nn.max_pool(norm4, ksize=[1, 3, 3, 1], strides=[1,\n",
        "         #                      2, 2, 1], padding='SAME', name='pool1')\n",
        "\n",
        "    # #################\n",
        "\n",
        "    with tf.variable_scope('flatten') as scope:\n",
        "    # Move everything into depth so we can perform a single matrix multiply.\n",
        "        reshape = tf.keras.layers.Flatten()(norm4)\n",
        "        dim = reshape.get_shape()[1].value\n",
        "        weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
        "                                              stddev=0.04, wd=0.004)\n",
        "        biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
        "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
        "        _activation_summary(local3)\n",
        "\n",
        "  # local4\n",
        "    with tf.variable_scope('local4') as scope:\n",
        "        weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
        "                                              stddev=0.04, wd=0.004)\n",
        "        biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
        "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
        "        _activation_summary(local4)\n",
        "\n",
        "  # linear layer(WX + b),\n",
        "  # We don't apply softmax here because\n",
        "  # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
        "  # and performs the softmax internally for efficiency.\n",
        "    with tf.variable_scope('softmax_linear') as scope:\n",
        "        weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n",
        "                                              stddev=1/192.0, wd=None)\n",
        "        biases = _variable_on_cpu('biases', [NUM_CLASSES],\n",
        "                                  tf.constant_initializer(0.0))\n",
        "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
        "        _activation_summary(softmax_linear)\n",
        "\n",
        "        return softmax_linear\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k83Z5pygZFD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def l2loss(logits, labels):\n",
        "    \"\"\"Add L2Loss to all the trainable variables.\n",
        "    Add summary for \"Loss\" and \"Loss/avg\".\n",
        "    Args:\n",
        "      logits: Logits from inference().\n",
        "      labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
        "              of shape [batch_size]\n",
        "    Returns:\n",
        "      Loss tensor of type float.\n",
        "    \"\"\"\n",
        "    # Calculate the average cross entropy loss across the batch.\n",
        "    labels = tf.cast(labels, tf.int64)\n",
        "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=labels, logits=logits, name='cross_entropy_per_example')\n",
        "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
        "    tf.add_to_collection('losses', cross_entropy_mean)\n",
        "    # The total loss is defined as the cross entropy loss plus all of the weight\n",
        "    # decay terms (L2 loss).\n",
        "    return tf.add_n(tf.get_collection('losses'), name='total_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-zjs635DBLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _add_loss_summaries(total_loss):\n",
        "  \"\"\"Add summaries for losses in CIFAR-10 model.\n",
        "  Generates moving average for all losses and associated summaries for\n",
        "  visualizing the performance of the network.\n",
        "  Args:\n",
        "    total_loss: Total loss from loss().\n",
        "  Returns:\n",
        "    loss_averages_op: op for generating moving averages of losses.\n",
        "  \"\"\"\n",
        "  # Compute the moving average of all individual losses and the total loss.\n",
        "  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
        "  losses = tf.get_collection('losses')\n",
        "  loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
        "\n",
        "  # Attach a scalar summary to all individual losses and the total loss; do the\n",
        "  # same for the averaged version of the losses.\n",
        "  for l in losses + [total_loss]:\n",
        "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
        "    # as the original loss name.\n",
        "    tf.summary.scalar(l.op.name + ' (raw)', l)\n",
        "    tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
        "\n",
        "  return loss_averages_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RNNZM20ZH5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(logits,labels):\n",
        "\n",
        "    acc, acc_op = tf.metrics.accuracy(labels=tf.argmax(labels, 1), \n",
        "                                  predictions=tf.argmax(logits,1))\n",
        "    \n",
        "    tf.add_to_collection('accuracy', acc)\n",
        "    \n",
        "    return acc, acc_op\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctu4_eS5DH5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(total_loss, batch_size):\n",
        "  \"\"\"Train CIFAR-10 model.\n",
        "  Create an optimizer and apply to all trainable variables. Add moving\n",
        "  average for all trainable variables.\n",
        "  Args:\n",
        "    total_loss: Total loss from loss().\n",
        "    global_step: Integer Variable counting the number of training steps\n",
        "      processed.\n",
        "  Returns:\n",
        "    train_op: op for training.\n",
        "  \"\"\"\n",
        "  # Variables that affect learning rate.\n",
        "\n",
        "  global_step=tf.train.get_or_create_global_step()\n",
        "  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n",
        "  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
        "\n",
        "  # Decay the learning rate exponentially based on the number of steps.\n",
        "  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
        "                                  global_step,\n",
        "                                  decay_steps,\n",
        "                                  LEARNING_RATE_DECAY_FACTOR,\n",
        "                                  staircase=True)\n",
        "  tf.summary.scalar('learning_rate', lr)\n",
        "\n",
        "  # Generate moving averages of all losses and associated summaries.\n",
        "  loss_averages_op = _add_loss_summaries(total_loss)\n",
        "\n",
        "  update_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "  # Compute gradients.\n",
        "  with tf.control_dependencies([loss_averages_op]):\n",
        "    opt = tf.train.GradientDescentOptimizer(lr)\n",
        "    \n",
        "    grads = opt.compute_gradients(total_loss)\n",
        "\n",
        "  # Apply gradients.\n",
        "  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
        "\n",
        "  # Add histograms for trainable variables.\n",
        "  for var in tf.trainable_variables():\n",
        "    tf.summary.histogram(var.op.name, var)\n",
        "\n",
        "  # Add histograms for gradients.\n",
        "  for grad, var in grads:\n",
        "    if grad is not None:\n",
        "      tf.summary.histogram(var.op.name + '/gradients', grad)\n",
        "\n",
        "  # Track the moving averages of all trainable variables.\n",
        "  variable_averages = tf.train.ExponentialMovingAverage(\n",
        "      MOVING_AVERAGE_DECAY, global_step)\n",
        "  with tf.control_dependencies([apply_gradient_op]):\n",
        "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
        "\n",
        "  return variables_averages_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1VmpB0JZL_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(total_loss, batch_size):\n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    num_examples_per_epoch_for_train = 50000 # Number of training samples\n",
        "    num_epochs_per_decay = 350.0\n",
        "    num_batches_per_epoch = num_examples_per_epoch_for_train / batch_size  # Calculate number of batches for each epoch\n",
        "    decay_steps = int(num_batches_per_epoch * num_epochs_per_decay)\n",
        "    lr = tf.train.exponential_decay(\n",
        "        0.1, global_step, decay_steps,\n",
        "        decay_rate=0.1, staircase=True)\n",
        "\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # To ensure anything in below code block is run already\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        train_op = tf.contrib.layers.optimize_loss(\n",
        "            loss=total_loss,\n",
        "            global_step=global_step,\n",
        "            learning_rate=lr,\n",
        "            optimizer='SGD')         \n",
        "    return train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92tp2YJpNGyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO8yVu6wZPi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CIFAR-10 consists of 50K training examples and 10K eval examples.\n",
        "# Each image has size 32x32 (and depth 3 for RGB).\n",
        "CIFAR_TRAIN_SIZE = 50000\n",
        "CIFAR_EVAL_SIZE = 10000\n",
        "CIFAR_IMAGE_SIZE = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0kbS-eHawHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def input_fn(data_dir,batch_size): # input parameter dir of cifar 10 data and batch size\n",
        "    # The src of data will be downloaded to pwd when running first time\n",
        "    filenames = glob(os.path.join(data_dir, 'cifar-10-batches-bin', 'data_batch_*.bin')) # List all filenames inside cifar-10-batches-bin directory and list all filenames matching data_batch_*.bin \n",
        "    pprint(filenames)\n",
        "\n",
        "    depth = 3\n",
        "    height = width = CIFAR_IMAGE_SIZE\n",
        "    label_bytes = 1\n",
        "    image_bytes = height * width * depth\n",
        "    # Every record consists of a label followed by the image,\n",
        "    # with a fixed number of bytes for each.\n",
        "    record_bytes = label_bytes + image_bytes\n",
        "\n",
        "    def decode_line(value): # Convert binary record into image tensor and it's corresponding label\n",
        "        \"\"\"Additional processing to perform on each line in dataset.\"\"\"\n",
        "        record_bytes = tf.decode_raw(value, tf.uint8) # 1. Convert each byte into integer representation\n",
        "        # The first bytes represent the label, which we convert from uint8->int32.\n",
        "        label = tf.to_int32(tf.strided_slice(record_bytes, [0], [label_bytes]))\n",
        "        # The remaining bytes after the label represent the image, which we reshape\n",
        "        # from [depth * height * width] to [depth, height, width].\n",
        "        depth_major = tf.reshape(\n",
        "            tf.strided_slice(record_bytes, [label_bytes], [label_bytes + image_bytes]),\n",
        "            [depth, height, width])\n",
        "        # Convert from [depth, height, width] to [height, width, depth].\n",
        "        uint8image = tf.transpose(depth_major, [1, 2, 0])\n",
        "        reshaped_image = tf.cast(uint8image, tf.float32)\n",
        "        # Randomly flip the image horizontally.\n",
        "        distorted_image = tf.image.random_flip_left_right(reshaped_image)\n",
        "        # Subtract off the mean and divide by the variance of the pixels.\n",
        "        float_image = tf.image.per_image_standardization(distorted_image)\n",
        "        # Set the shapes of tensors.\n",
        "        float_image.set_shape([height, width, 3])\n",
        "        label.set_shape([1])\n",
        "        return float_image, label\n",
        "\n",
        "    # Repeat infinitely.\n",
        "    dataset = tf.data.FixedLengthRecordDataset(filenames, record_bytes).repeat() # expects list of filenames and number of bytes each record takes \n",
        "    dataset = dataset.map(decode_line, num_parallel_calls=batch_size) # Converts binary record into image tensor\n",
        "\n",
        "    min_fraction_of_examples_in_queue = 0.4\n",
        "    min_queue_examples = int(CIFAR_TRAIN_SIZE * min_fraction_of_examples_in_queue)\n",
        "    dataset = dataset.shuffle(buffer_size=min_queue_examples + 3 * batch_size) # Customise data shuffle in input pipeline\n",
        "\n",
        "    # Batch it up.\n",
        "    dataset = dataset.batch(batch_size) # Set batch size\n",
        "    iterator = dataset.make_one_shot_iterator() # Iterate over batches returning image batch, label pairs\n",
        "    image_batch, label_batch = iterator.get_next() # Retrieves sequence of batches\n",
        "    # Ensure we don't have any shape dimensions equal to None...\n",
        "    image_batch.set_shape([batch_size, height, width, 3])\n",
        "    label_batch = tf.squeeze(label_batch)\n",
        "    return image_batch, label_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZAfjSEJazWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def model_fn(features, labels, mode):\n",
        "     logits = inference(\n",
        "        image_batch=features,\n",
        "        batch_size=batch_size)\n",
        "    \n",
        "     loss =l2loss(logits, labels)\n",
        "     \n",
        "     train_op =train(loss, batch_size=batch_size)\n",
        "\n",
        "     if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        logging_hook = tf.train.LoggingTensorHook({'loss': loss}, every_n_iter=1)\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode,\n",
        "            loss=loss,\n",
        "            train_op=train_op,\n",
        "            \n",
        "            training_hooks=[logging_hook])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K81yyBiAa8eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # Ensure data_dir has dataset and model_dir is cleared before training.\n",
        "    maybe_download_and_extract(data_dir=data_dir) # Download data if not exist in data dir specified\n",
        "    if tf.gfile.Exists(model_dir):\n",
        "        tf.gfile.DeleteRecursively(model_dir)\n",
        "    tf.gfile.MakeDirs(model_dir)  # Clear model dir before instatiating\n",
        "\n",
        "    \n",
        "    classifier = tf.estimator.Estimator(\n",
        "        model_fn=model_fn,\n",
        "        model_dir=model_dir)\n",
        "    classifier.train(\n",
        "        input_fn=lambda: input_fn(data_dir, batch_size),\n",
        "        steps=20) # Train classifier calls model_fn build entire model graph  look past input function and do training updates used simple callable a funtion can be called without param by wrapping in lambda\n",
        "   \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDtA7_0Ta_U1",
        "colab_type": "code",
        "outputId": "1e227fc8-d94c-4156-9aba-2f7fef0aafb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "I0816 12:34:36.182737 140185262561152 estimator.py:1790] Using default config.\n",
            "I0816 12:34:36.185557 140185262561152 estimator.py:209] Using config: {'_model_dir': 'models/cifar', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7f224e4828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "W0816 12:34:36.199561 140185262561152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0816 12:34:36.246605 140185262561152 deprecation.py:323] From <ipython-input-22-96d0a23d7c74>:21: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0816 12:34:36.301131 140185262561152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0816 12:34:36.321468 140185262561152 deprecation.py:323] From <ipython-input-22-96d0a23d7c74>:49: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "I0816 12:34:36.346169 140185262561152 estimator.py:1145] Calling model_fn.\n",
            "W0816 12:34:36.347907 140185262561152 deprecation.py:506] From <ipython-input-11-97d93e3d539c>:19: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['data/cifar/cifar-10-batches-bin/data_batch_5.bin',\n",
            " 'data/cifar/cifar-10-batches-bin/data_batch_1.bin',\n",
            " 'data/cifar/cifar-10-batches-bin/data_batch_2.bin',\n",
            " 'data/cifar/cifar-10-batches-bin/data_batch_4.bin',\n",
            " 'data/cifar/cifar-10-batches-bin/data_batch_3.bin']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0816 12:34:37.743663 140185262561152 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I0816 12:34:37.918793 140185262561152 estimator.py:1147] Done calling model_fn.\n",
            "I0816 12:34:37.922474 140185262561152 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "W0816 12:34:38.063102 140185262561152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0816 12:34:38.159185 140185262561152 monitored_session.py:240] Graph was finalized.\n",
            "I0816 12:34:39.913026 140185262561152 session_manager.py:500] Running local_init_op.\n",
            "I0816 12:34:39.927798 140185262561152 session_manager.py:502] Done running local_init_op.\n",
            "I0816 12:34:40.291697 140185262561152 basic_session_run_hooks.py:606] Saving checkpoints for 0 into models/cifar/model.ckpt.\n",
            "I0816 12:34:50.946367 140185262561152 basic_session_run_hooks.py:262] loss = 18.06748, step = 0\n",
            "I0816 12:34:50.948701 140185262561152 basic_session_run_hooks.py:262] loss = 18.06748\n",
            "I0816 12:34:51.286574 140185262561152 basic_session_run_hooks.py:260] loss = 18.532505 (0.338 sec)\n",
            "I0816 12:34:51.512642 140185262561152 basic_session_run_hooks.py:260] loss = 18.052647 (0.226 sec)\n",
            "I0816 12:34:51.735183 140185262561152 basic_session_run_hooks.py:260] loss = 18.02021 (0.223 sec)\n",
            "I0816 12:34:51.956590 140185262561152 basic_session_run_hooks.py:260] loss = 17.995897 (0.221 sec)\n",
            "I0816 12:34:52.188446 140185262561152 basic_session_run_hooks.py:260] loss = 17.978186 (0.232 sec)\n",
            "I0816 12:34:52.422752 140185262561152 basic_session_run_hooks.py:260] loss = 17.952559 (0.234 sec)\n",
            "I0816 12:34:52.660173 140185262561152 basic_session_run_hooks.py:260] loss = 17.943768 (0.237 sec)\n",
            "I0816 12:34:52.885468 140185262561152 basic_session_run_hooks.py:260] loss = 17.913864 (0.225 sec)\n",
            "I0816 12:34:53.112940 140185262561152 basic_session_run_hooks.py:260] loss = 17.904295 (0.227 sec)\n",
            "I0816 12:34:53.340855 140185262561152 basic_session_run_hooks.py:260] loss = 17.880754 (0.228 sec)\n",
            "I0816 12:34:53.564431 140185262561152 basic_session_run_hooks.py:260] loss = 17.945168 (0.224 sec)\n",
            "I0816 12:34:53.796292 140185262561152 basic_session_run_hooks.py:260] loss = 17.850075 (0.232 sec)\n",
            "I0816 12:34:54.005896 140185262561152 basic_session_run_hooks.py:260] loss = 17.81096 (0.210 sec)\n",
            "I0816 12:34:54.239075 140185262561152 basic_session_run_hooks.py:260] loss = 17.775362 (0.233 sec)\n",
            "I0816 12:34:54.451583 140185262561152 basic_session_run_hooks.py:260] loss = 17.733692 (0.213 sec)\n",
            "I0816 12:34:54.670609 140185262561152 basic_session_run_hooks.py:260] loss = 17.728508 (0.219 sec)\n",
            "I0816 12:34:54.899151 140185262561152 basic_session_run_hooks.py:260] loss = 17.63585 (0.229 sec)\n",
            "I0816 12:34:55.112893 140185262561152 basic_session_run_hooks.py:260] loss = 18.441221 (0.214 sec)\n",
            "I0816 12:34:55.338271 140185262561152 basic_session_run_hooks.py:260] loss = 19.266768 (0.225 sec)\n",
            "I0816 12:34:55.343835 140185262561152 basic_session_run_hooks.py:606] Saving checkpoints for 20 into models/cifar/model.ckpt.\n",
            "I0816 12:34:55.629310 140185262561152 estimator.py:368] Loss for final step: 19.266768.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn4NNsHLGOIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S088QtP-edd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}